{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_next_state(current, action, dim):\n",
    "    x, y = current\n",
    "    coord = None\n",
    "    \n",
    "    if action == 'up':\n",
    "        coord = (x-1, y) if x-1 >= 0 else (x, y)\n",
    "    elif action == 'down':\n",
    "        coord = (x+1, y) if x+1 < dim else (x, y)\n",
    "    elif action == 'left':\n",
    "        coord = (x, y-1) if y-1 >= 0 else (x, y)\n",
    "    elif action == 'right':\n",
    "        coord = (x, y+1) if y+1 < dim else (x, y)\n",
    "    \n",
    "    return coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is the algorithm from Chapter 4.1 Policy Evaluation policy\n",
    "Iterative Policy Evaluation\n",
    "'''\n",
    "def eval_policy(grid, grid_policy, theta):\n",
    "    # 1. init the values\n",
    "    terminal_states = [ (0,0), (3, 3) ]\n",
    "\n",
    "    while True: \n",
    "        delta = 0.0\n",
    "        state_iter = np.nditer(grid, flags=['multi_index'])\n",
    "\n",
    "        for item in state_iter:\n",
    "            old_value = item\n",
    "            x, y = state_iter.multi_index\n",
    "\n",
    "            if (x, y) in terminal_states:\n",
    "                continue\n",
    "            \n",
    "            policy = grid_policy[(x, y)]\n",
    "            value = 0.0\n",
    "            for action, prob in policy.items():\n",
    "                next_state_x, next_state_y = compute_next_state((x, y), action, 4)\n",
    "                value += prob * 1 * (-1 + grid[next_state_x][next_state_y])\n",
    "\n",
    "            delta = max(delta, abs(value - old_value))\n",
    "            grid[x][y] = value\n",
    "\n",
    "        if delta < theta: \n",
    "            break\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_max_action(action_value_map): \n",
    "    max_value = max(action_value_map.values())\n",
    "    \n",
    "    max_actions = []\n",
    "    for k, v in action_value_map.items():\n",
    "        if v == max_value:\n",
    "            max_actions.append(k)\n",
    "    \n",
    "    for k, v in action_value_map.items():\n",
    "        if k in max_actions:\n",
    "            action_value_map[k] = 1.0/len(max_actions)\n",
    "        else:\n",
    "            action_value_map[k] = 0\n",
    "    return action_value_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printif(state, msg):\n",
    "    if state == (1, 2):\n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_policy(grid, grid_policy):\n",
    "    state_iter = np.nditer(grid, flags=['multi_index'])\n",
    "    terminal_states = [ (0,0), (3, 3) ]\n",
    "    \n",
    "    for value in state_iter:\n",
    "        state = state_iter.multi_index\n",
    "        \n",
    "        if state in terminal_states:\n",
    "            continue\n",
    "        \n",
    "        # policy for this state\n",
    "        old_policy = grid_policy[state]\n",
    "        \n",
    "        action_value_map = {}\n",
    "        \n",
    "        for action, prob in old_policy.items():\n",
    "            _x, _y = compute_next_state(state, action, 4)\n",
    "            action_value = 1 * (-1 + grid[_x][_y])\n",
    "            action_value_map[action] = action_value\n",
    "            \n",
    "        new_policy = compute_max_action(action_value_map)\n",
    "        \n",
    "        # this is slightly different from the algorithm in the book. In the book, it compares original action\n",
    "        # vs the new action which is the argmax of Q. Since we are evaluating for stochastic case here, we have to\n",
    "        # compare original policy vs updated policy for this particular state.\n",
    "        \n",
    "        if old_policy != new_policy:\n",
    "            # we need to update our policy\n",
    "            grid_policy[state] = new_policy\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the values\n",
    "grid = np.zeros((4, 4))\n",
    "theta = 0.05\n",
    "\n",
    "policy = { \n",
    "    'up': 0.25,\n",
    "    'down': 0.25,\n",
    "    'left': 0.25,\n",
    "    'right': 0.25\n",
    "}\n",
    "\n",
    "# create policy map for the whole grid\n",
    "grid_policy = {}\n",
    "state_iter = np.nditer(grid, flags=['multi_index'])\n",
    "for value in state_iter:\n",
    "    state = state_iter.multi_index\n",
    "    grid_policy[state] = policy\n",
    "\n",
    "policy_stable = False\n",
    "\n",
    "while not policy_stable:\n",
    "    # eval\n",
    "    grid = eval_policy(grid, grid_policy, theta)\n",
    "    \n",
    "    # improve\n",
    "    policy_stable = improve_policy(grid, grid_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " (0, 1): {'up': 0, 'down': 0, 'left': 1.0, 'right': 0},\n",
       " (0, 2): {'up': 0, 'down': 0, 'left': 1.0, 'right': 0},\n",
       " (0, 3): {'up': 0, 'down': 0.5, 'left': 0.5, 'right': 0},\n",
       " (1, 0): {'up': 1.0, 'down': 0, 'left': 0, 'right': 0},\n",
       " (1, 1): {'up': 0.5, 'down': 0, 'left': 0.5, 'right': 0},\n",
       " (1, 2): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " (1, 3): {'up': 0, 'down': 1.0, 'left': 0, 'right': 0},\n",
       " (2, 0): {'up': 1.0, 'down': 0, 'left': 0, 'right': 0},\n",
       " (2, 1): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " (2, 2): {'up': 0, 'down': 0.5, 'left': 0, 'right': 0.5},\n",
       " (2, 3): {'up': 0, 'down': 1.0, 'left': 0, 'right': 0},\n",
       " (3, 0): {'up': 0.5, 'down': 0, 'left': 0, 'right': 0.5},\n",
       " (3, 1): {'up': 0, 'down': 0, 'left': 0, 'right': 1.0},\n",
       " (3, 2): {'up': 0, 'down': 0, 'left': 0, 'right': 1.0},\n",
       " (3, 3): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
